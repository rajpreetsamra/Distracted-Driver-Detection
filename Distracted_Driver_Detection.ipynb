{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0b6003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/Deep Learning and Neural Networks/Distracted Driver'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a98675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a47e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab9602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69122788",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"/Users/apple/Deep Learning and Neural Networks/Distracted Driver\"+\"/state-farm-distracted-driver-detection/imgs/train\"\n",
    "train_dir\n",
    "test_dir=\"/Users/apple/Deep Learning and Neural Networks/Distracted Driver\"+\"/state-farm-distracted-driver-detection/imgs/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71dfcdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=train_dir+\"/0\"+\"/img_100026.jpg\"\n",
    "image1=cv.imread(image)\n",
    "gray=cv.cvtColor(image1,cv.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe65389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd885fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1e0321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cv.imshow(\"ex\",image1)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a69f150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blur=cv.GaussianBlur(image1,(3,3),cv.BORDER_DEFAULT)\n",
    "cv.imshow(\"blur\",blur)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38a1e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a07130a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canny=cv.Canny(image1,125,175)\n",
    "canny1=cv.Canny(blur,125,175)\n",
    "cv.imshow(\"canny\",canny)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "774fdd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dilated=cv.dilate(canny,(3,3),iterations=1)\n",
    "cv.imshow(\"dilated\",dilated)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c135fe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eroded=cv.erode(dilated,(3,3),iterations=1)\n",
    "cv.imshow(\"eroded\",eroded)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db81dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b190df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret,thresh=cv.threshold(canny,125,255,cv.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee0f0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"thresh\",thresh)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a640052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835 contours(s) found?\n"
     ]
    }
   ],
   "source": [
    "contours, hierarchies=cv.findContours(thresh,cv.RETR_LIST,cv.CHAIN_APPROX_NONE)\n",
    "print(f'{len(contours)} contours(s) found?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2992d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "blank=np.zeros(image1.shape,dtype='uint8')\n",
    "cv.drawContours(blank,contours,-1,(0,0,255),2)\n",
    "cv.imshow(\"contours drawn\",blank)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e39172d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bilateral Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa2b2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilateral=cv.bilateralFilter(image1,5,15,15)\n",
    "cv.imshow(\"bilateral\",bilateral)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "334264cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "canny2=cv.Canny(bilateral,125,175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e96329d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0781ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray=cv.cvtColor(image1,cv.COLOR_BGR2GRAY)\n",
    "cv.imshow(\"simple threshold\",gray)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3031acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef6a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold,thresh_inv=cv.threshold(gray,150,255,cv.THRESH_BINARY_INV)\n",
    "cv.imshow(\"simple_threshold_inv\",thresh_inv)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "229d1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaptive threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a675784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_thresh=cv.adaptiveThreshold(gray,255,cv.ADAPTIVE_THRESH_MEAN_C,cv.THRESH_BINARY,11,3)\n",
    "cv.imshow(\"adaptive\",adaptive_thresh)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e79dcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradients and edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33df1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gray=cv.cvtColor(image1,cv.COLOR_BGR2GRAY)\n",
    "lap=cv.Laplacian(gray,cv.CV_64F)\n",
    "lap=np.uint8(np.absolute(lap))\n",
    "cv.imshow(\"laplacian\",lap)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf75dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d77cb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelx=cv.Sobel(gray,cv.CV_64F,1,0)\n",
    "sobely=cv.Sobel(gray,cv.CV_64F,0,1)\n",
    "combined_sobel=cv.bitwise_or(sobelx,sobely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfda4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"sobelx\",sobelx)\n",
    "cv.imshow(\"sobely\",sobely)\n",
    "cv.imshow(\"sobel_combined\",combined_sobel)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30c72577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09d7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_gen=ImageDataGenerator(rescale=1.0/255,validation_split=0.2)\n",
    "validation_data_gen=ImageDataGenerator(rescale=1.0/255,validation_split=0.2)\n",
    "test_data_gen=ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7b1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049c6117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22424 files belonging to 11 classes.\n",
      "Using 17940 files for training.\n",
      "Found 22424 files belonging to 11 classes.\n",
      "Using 4484 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_data=tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                      validation_split=0.2,\n",
    "                                                      subset=\"training\",\n",
    "                                                      image_size=(100,100),\n",
    "                                                       seed=123,\n",
    "                                                      batch_size=128,\n",
    "                                                      label_mode=\"categorical\")\n",
    "validation_data=tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                           validation_split=0.2,\n",
    "                                                           subset=\"validation\",\n",
    "                                                           image_size=(100,100),\n",
    "                                                            seed=123,\n",
    "                                                          batch_size=128,\n",
    "                                                          label_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad28f676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 11 classes.\n",
      "Found 4481 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "training_generator=train_data_gen.flow_from_directory(train_dir,\n",
    "                                                     target_size=(100,100),\n",
    "                                                     batch_size=128,\n",
    "                                                     class_mode=\"categorical\",\n",
    "                                                     subset=\"training\")\n",
    "validation_generator=validation_data_gen.flow_from_directory(train_dir,\n",
    "                                                            target_size=(100,100),\n",
    "                                                            batch_size=128,\n",
    "                                                            class_mode=\"categorical\",\n",
    "                                                            subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40cfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df696a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41071d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06a9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "747ba562",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(64,(3,3),activation=\"relu\",input_shape=(100,100,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3),activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512,activation=\"relu\"))\n",
    "model.add(layers.Dense(11,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfff76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),loss=\"categorical_crossentropy\",metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.3370 - acc: 0.5643"
     ]
    }
   ],
   "source": [
    "history=model.fit(training_generator,\n",
    "                 #steps_per_epoch=230,\n",
    "                 epochs=10,\n",
    "                 validation_data=validation_generator)\n",
    "                 #validation_steps=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fe760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b189ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
